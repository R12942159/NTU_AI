{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r12942159/miniconda3/envs/AI/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import evaluate\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoProcessor, AutoModelForVision2Seq, AutoModelForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define models and processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r12942159/miniconda3/envs/AI/lib/python3.13/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/home/r12942159/.cache/huggingface/modules/transformers_modules/microsoft/Phi-4-multimodal-instruct/18812f44eec22f4347a85536d97059356bd5ec2f/speech_conformer_encoder.py:2775: FutureWarning: Please specify CheckpointImpl.NO_REENTRANT as CheckpointImpl.REENTRANT will soon be removed as the default and eventually deprecated.\n",
      "  lambda i: encoder_checkpoint_wrapper(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.61it/s]\n"
     ]
    }
   ],
   "source": [
    "custom_cache_dir = \"/home/r12942159/data_18TB\"\n",
    "blip_model_name = \"Salesforce/blip-image-captioning-base\"\n",
    "phi4_model_name = \"microsoft/Phi-4-multimodal-instruct\"\n",
    "\n",
    "blip_processor = BlipProcessor.from_pretrained(blip_model_name, cache_dir=custom_cache_dir)\n",
    "blip_model = BlipForConditionalGeneration.from_pretrained(blip_model_name, cache_dir=custom_cache_dir).to(\"cuda\")\n",
    "\n",
    "phi4_processor = AutoProcessor.from_pretrained(phi4_model_name, trust_remote_code=True, cache_dir=custom_cache_dir)\n",
    "phi4_model = AutoModelForCausalLM.from_pretrained(\n",
    "    phi4_model_name, \n",
    "    device_map=\"cuda\", \n",
    "    torch_dtype=\"auto\", \n",
    "    trust_remote_code=True,\n",
    "    _attn_implementation='flash_attention_2',\n",
    "    cache_dir=custom_cache_dir,\n",
    ").cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_cache_dir = '/home/r12942159/data_18TB/datasets'\n",
    "\n",
    "datasets = {\n",
    "    \"MSCOCO-Test\": load_dataset(\"nlphuji/mscoco_2014_5k_test_image_text_retrieval\",\n",
    "                                cache_dir=datasets_cache_dir),\n",
    "    \"Flickr30k\": load_dataset(\"nlphuji/flickr30k\",\n",
    "                              cache_dir=datasets_cache_dir)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_captioning(model, processor, image, model_name):\n",
    "    if model_name == \"BLIP\":\n",
    "        inputs = processor(images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs)\n",
    "            caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "    elif model_name == \"Phi-4\":\n",
    "        generation_config = GenerationConfig.from_pretrained(phi4_model_name)\n",
    "        prompt = [\"<|user|><|image_1|>Describe the image in detail.<|end|><|assistant|>\" for _ in range(len(image))]\n",
    "        inputs = processor(text=prompt, images=image, return_tensors='pt').to('cuda:0')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                            **inputs,\n",
    "                            max_new_tokens=1000,\n",
    "                            generation_config=generation_config,\n",
    "                            )\n",
    "            generated_ids = generated_ids[:, inputs['input_ids'].shape[1]:]\n",
    "            caption = processor.batch_decode(generated_ids, \n",
    "                                    skip_special_tokens=True,\n",
    "                                    clean_up_tokenization_spaces=False,)\n",
    "    \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Compute evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入評估指標\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "rouge = evaluate.load(\"rouge\", rouge_types=[\"rouge1\", \"rouge2\"])\n",
    "meteor = evaluate.load(\"meteor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(references, prediction):\n",
    "    smoothie = SmoothingFunction().method1\n",
    "    bleu_score = sentence_bleu([ref.split() for ref in references], prediction.split(), smoothing_function=smoothie)\n",
    "\n",
    "    rouge = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\"], use_stemmer=True)\n",
    "    rouge_scores = [rouge.score(ref, prediction) for ref in references]\n",
    "\n",
    "    best_rouge1 = max(score[\"rouge1\"].fmeasure for score in rouge_scores)\n",
    "    best_rouge2 = max(score[\"rouge2\"].fmeasure for score in rouge_scores)\n",
    "\n",
    "    meteor_score = meteor.compute(predictions=[prediction], references=[references])[\"meteor\"]\n",
    "\n",
    "    return {\n",
    "        \"BLEU\": bleu_score,\n",
    "        \"ROUGE-1\": best_rouge1,\n",
    "        \"ROUGE-2\": best_rouge2,\n",
    "        \"METEOR\": float(meteor_score),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Process images from datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing MSCOCO-Test: 100%|██████████| 5000/5000 [4:39:26<00:00,  3.35s/it]   \n",
      "Processing Flickr30k: 100%|██████████| 31014/31014 [30:34:53<00:00,  3.55s/it]   \n"
     ]
    }
   ],
   "source": [
    "# results = {}\n",
    "\n",
    "# for dataset_name, dataset in datasets.items():\n",
    "#     results[dataset_name] = {}\n",
    "#     if dataset_name == \"MSCOCO-Test\":\n",
    "#         img_id = \"cocoid\"\n",
    "#     else:\n",
    "#         img_id = \"img_id\"\n",
    "        \n",
    "#     for sample in tqdm(dataset['test'], desc=f\"Processing {dataset_name}\"):\n",
    "#         image = sample['image']\n",
    "#         gt_caption = sample[\"caption\"]\n",
    "        \n",
    "#         blip_caption = evaluate_captioning(blip_model, blip_processor, image, \"BLIP\")\n",
    "#         phi4_caption = evaluate_captioning(phi4_model, phi4_processor, image, \"Phi-4\")\n",
    "\n",
    "#         results[dataset_name][sample[img_id]] = {\n",
    "#             \"GT\": gt_caption,\n",
    "#             \"BLIP\": blip_caption,\n",
    "#             \"Phi-4\": phi4_caption,\n",
    "#             \"Metrics\": {\n",
    "#                 \"BLIP\": compute_metrics(gt_caption, blip_caption),\n",
    "#                 \"Phi-4\": compute_metrics(gt_caption, phi4_caption)\n",
    "#             }\n",
    "#         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "results = {}\n",
    "batch_size = 8\n",
    "\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    results[dataset_name] = {}\n",
    "\n",
    "    if dataset_name == \"MSCOCO-Test\":\n",
    "        img_id = \"cocoid\"\n",
    "    else:\n",
    "        img_id = \"img_id\"\n",
    "\n",
    "    dataset_samples = dataset['test']\n",
    "    num_samples = len(dataset_samples) # 5000\n",
    "\n",
    "    elapsed_time = - time.time()\n",
    "\n",
    "    for i in tqdm(range(0, num_samples, batch_size), desc=f\"Processing {dataset_name} in Batches\"):\n",
    "        batch_samples = dataset_samples[i : i + batch_size]  # 取出 batch\n",
    "        batch_images = [sample for sample in batch_samples['image']]\n",
    "        batch_gt_captions = [sample for sample in batch_samples[\"caption\"]]\n",
    "        batch_ids = [sample for sample in batch_samples[img_id]]\n",
    "\n",
    "        blip_captions = evaluate_captioning(blip_model, blip_processor, batch_images, \"BLIP\")\n",
    "        phi4_caption = evaluate_captioning(phi4_model, phi4_processor, batch_images, \"Phi-4\")\n",
    "\n",
    "        for j in range(len(batch_ids)):\n",
    "            results[dataset_name][batch_ids[j]] = {\n",
    "                \"GT\": batch_gt_captions[j],\n",
    "                \"BLIP\": blip_captions[j],\n",
    "                \"Phi-4\": phi4_caption[j],\n",
    "                \"Metrics\": {\n",
    "                    \"BLIP\": compute_metrics(batch_gt_captions[j], blip_captions[j]),\n",
    "                    \"Phi-4\": compute_metrics(batch_gt_captions[j], phi4_caption[j]),\n",
    "                }\n",
    "            }\n",
    "\n",
    "    elapsed_time += time.time()\n",
    "    print(f\"Time taken for {dataset_name}: {elapsed_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# with open(\"./part1_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     json.dump(results, f, ensure_ascii=False, indent=4)  # `ensure_ascii=False` 確保中文不轉換為 Unicode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMSCOCO-Test\t\t\tFlickr30k\n",
      "\tBLEU\tROUGE-1\tROUGE-2\tMETEOR\tBLEU\tROUGE-1\tROUGE-2\tMETEOR\n",
      "BLIP\t0.2052\t0.5830\t0.3448\t0.4207\t0.1432\t0.4932\t0.2672\t0.3232\n",
      "Phi-4\t0.0323\t0.1982\t0.0931\t0.3077\t0.0293\t0.2131\t0.0899\t0.3034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def compute_avg_scores(results, dataset_name):\n",
    "    metrics = {\"BLIP\": {\"BLEU\": [], \"ROUGE-1\": [], \"ROUGE-2\": [], \"METEOR\": []},\n",
    "               \"Phi-4\": {\"BLEU\": [], \"ROUGE-1\": [], \"ROUGE-2\": [], \"METEOR\": []}}\n",
    "    \n",
    "    for sample in results.get(dataset_name, {}).values():\n",
    "        if \"Metrics\" in sample:\n",
    "            for model in [\"BLIP\", \"Phi-4\"]:\n",
    "                metrics[model][\"BLEU\"].append(sample[\"Metrics\"][model][\"BLEU\"])\n",
    "                metrics[model][\"ROUGE-1\"].append(sample[\"Metrics\"][model][\"ROUGE-1\"])\n",
    "                metrics[model][\"ROUGE-2\"].append(sample[\"Metrics\"][model][\"ROUGE-2\"])\n",
    "                metrics[model][\"METEOR\"].append(sample[\"Metrics\"][model][\"METEOR\"])\n",
    "    \n",
    "    avg_metrics = {\n",
    "        model: {k: np.mean(v) if v else 0 for k, v in metrics[model].items()}\n",
    "        for model in [\"BLIP\", \"Phi-4\"]\n",
    "    }\n",
    "    \n",
    "    return avg_metrics\n",
    "\n",
    "mscoco_avg = compute_avg_scores(results, \"MSCOCO-Test\")\n",
    "flickr_avg = compute_avg_scores(results, \"Flickr30k\")\n",
    "\n",
    "print(\"\\tMSCOCO-Test\\t\\t\\tFlickr30k\")\n",
    "print(\"\\tBLEU\\tROUGE-1\\tROUGE-2\\tMETEOR\\tBLEU\\tROUGE-1\\tROUGE-2\\tMETEOR\")\n",
    "for model in [\"BLIP\", \"Phi-4\"]:\n",
    "    print(f\"{model}\\t{mscoco_avg[model]['BLEU']:.4f}\\t{mscoco_avg[model]['ROUGE-1']:.4f}\\t\"\n",
    "          f\"{mscoco_avg[model]['ROUGE-2']:.4f}\\t{mscoco_avg[model]['METEOR']:.4f}\\t\"\n",
    "          f\"{flickr_avg[model]['BLEU']:.4f}\\t{flickr_avg[model]['ROUGE-1']:.4f}\\t\"\n",
    "          f\"{flickr_avg[model]['ROUGE-2']:.4f}\\t{flickr_avg[model]['METEOR']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
